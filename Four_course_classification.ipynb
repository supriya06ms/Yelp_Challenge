{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Yelp Dataset from json format to csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import argparse\n",
    "import simplejson as json\n",
    "\n",
    "\n",
    "def read_and_write_file(json_file_path, csv_file_path, column_names):\n",
    "    #Read in the json dataset file and write it out to a csv file, given the column names.\n",
    "    with open(csv_file_path, 'wb+') as fout:\n",
    "        csv_file = csv.writer(fout)\n",
    "        csv_file.writerow(list(column_names))\n",
    "        with open(json_file_path) as fin:\n",
    "            for line in fin:\n",
    "                line_contents = json.loads(line)\n",
    "                csv_file.writerow(get_row(line_contents, column_names))\n",
    "\n",
    "def get_superset_of_column_names_from_file(json_file_path):\n",
    "    #Read in the json dataset file and return the superset of column names.\n",
    "    column_names = set()\n",
    "    with open(json_file_path) as fin:\n",
    "        for line in fin:\n",
    "            line_contents = json.loads(line)\n",
    "            column_names.update(\n",
    "                    set(get_column_names(line_contents).keys())\n",
    "                    )\n",
    "    return column_names\n",
    "\n",
    "def get_column_names(line_contents, parent_key=''):\n",
    "    #Return a list of flattened key names given a dict\n",
    "    column_names = []\n",
    "    for k, v in line_contents.iteritems():\n",
    "        column_name = \"{0}.{1}\".format(parent_key, k) if parent_key else k\n",
    "        if isinstance(v, collections.MutableMapping):\n",
    "            column_names.extend(\n",
    "                    get_column_names(v, column_name).items()\n",
    "                    )\n",
    "        else:\n",
    "            column_names.append((column_name, v))\n",
    "    return dict(column_names)\n",
    "\n",
    "def get_nested_value(d, key):\n",
    "    #Return a dictionary item given a dictionary `d` and a flattened key from `get_column_names`\n",
    "    \n",
    "    \n",
    "    if '.' not in key:\n",
    "        if key not in d:\n",
    "            return None\n",
    "        return d[key]\n",
    "    base_key, sub_key = key.split('.', 1)\n",
    "    if base_key not in d:\n",
    "        return None\n",
    "    sub_dict = d[base_key]\n",
    "    return get_nested_value(sub_dict, sub_key)\n",
    "\n",
    "def get_row(line_contents, column_names):\n",
    "    #Return a csv compatible row given column names and a dict.\n",
    "    row = []\n",
    "    for column_name in column_names:\n",
    "        line_value = get_nested_value(\n",
    "                        line_contents,\n",
    "                        column_name,\n",
    "                        )\n",
    "        if isinstance(line_value, unicode):\n",
    "            row.append('{0}'.format(line_value.encode('utf-8')))\n",
    "        elif line_value is not None:\n",
    "            row.append('{0}'.format(line_value))\n",
    "        else:\n",
    "            row.append('')\n",
    "    return row\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Convert a yelp dataset file from json to csv.\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "            description='Convert Yelp Dataset from JSON format to CSV.',\n",
    "            )\n",
    "\n",
    "    parser.add_argument(\n",
    "            'json_file',\n",
    "            type=str,\n",
    "            help='The json file to convert.',\n",
    "            )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    json_file = args.json_file\n",
    "    csv_file = '{0}.csv'.format(json_file.split('.json')[0])\n",
    "\n",
    "    column_names = get_superset_of_column_names_from_file(json_file)\n",
    "read_and_write_file(json_file, csv_file, column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction: \n",
    "\n",
    "### Extract the data pertaining to \"Restaurants\"\n",
    "Yelp dataset contains the data of various business categories like University, Hospital, Malls, Restaurants, Automobiles etc. Here as we are analysing the \"Restaurants\" data, the tuples of Restaurant category is extracted and stored as a separate csv file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "fieldnames = [\"attributes.Ambience.divey\", \"attributes.RestaurantsDelivery\",\"attributes.DogsAllowed\",\"postal_code\",\"hours.Thursday\",\"attributes.HairSpecializesIn.coloring\",\"attributes.BestNights.sunday\",\"attributes.BYOB\",\"attributes.AgesAllowed\",\"attributes.Music.video\",\"hours.Friday\",\"latitude\",\"attributes.Alcohol\",\"attributes.Ambience.classy\",\"attributes.RestaurantsTableService\",\"business_id\",\"attributes.Ambience.touristy\",\"attributes.RestaurantsCounterService\",\"attributes.Corkage\",\"attributes.RestaurantsGoodForGroups\",\"categories\",\"name\",\"attributes.BusinessAcceptsBitcoin\",\"attributes.HappyHour\",\"attributes.WheelchairAccessible\",\"attributes.Ambience.hipster\",\"attributes.BusinessAcceptsCreditCards\",\"is_open\",\"attributes.DietaryRestrictions.vegetarian\",\"attributes.Music.live\",\"attributes.Music.background_music\",\"neighborhood\",\"attributes.BusinessParking.lot\",\"attributes.Music.karaoke\",\"review_count\",\"attributes.GoodForMeal.breakfast\",\"attributes.NoiseLevel\",\"attributes.HairSpecializesIn.perms\",\"state\",\"attributes.DriveThru\",\"attributes.HasTV\",\"attributes.GoodForMeal.dinner\",\"attributes.BusinessParking.street\",\"address\",\"attributes.RestaurantsAttire\",\"hours.Sunday\",\"attributes.BestNights.tuesday\",\"attributes.AcceptsInsurance\",\"attributes.BestNights.wednesday\",\"hours.Wednesday\",\"attributes.HairSpecializesIn.kids\",\"attributes.Open24Hours\",\"attributes.Ambience.trendy\",\"attributes.CoatCheck\",\"hours.Monday\",\"attributes.HairSpecializesIn.straightperms\",\"city\",\"attributes.HairSpecializesIn.curly\",\"attributes.Music.no_music\",\"hours.Tuesday\",\"attributes.HairSpecializesIn.africanamerican\",\"stars\",\"attributes.RestaurantsPriceRange2\",\"attributes.Ambience.intimate\",\"attributes.GoodForMeal.latenight\",\"attributes.GoodForMeal.dessert\",\"attributes.BusinessParking.validated\",\"attributes.GoodForMeal.lunch\",\"attributes.GoodForKids\",\"attributes.DietaryRestrictions.soy-free\",\"attributes.GoodForMeal.brunch\",\"attributes.BusinessParking.valet\",\"longitude\",\"attributes.DietaryRestrictions.gluten-free\",\"attributes.BYOBCorkage\",\"attributes.BusinessParking.garage\",\"attributes.BestNights.friday\",\"hours.Saturday\",\"attributes.Music.dj\",\"attributes.HairSpecializesIn.extensions\",\"attributes.BestNights.saturday\",\"attributes.Ambience.casual\",\"attributes.BestNights.thursday\",\"attributes.BestNights.monday\",\"attributes.HairSpecializesIn.asian\",\"attributes.DietaryRestrictions.kosher\",\"attributes.WiFi\",\"attributes.Smoking\",\"attributes.DietaryRestrictions.halal\",\"attributes.GoodForDancing\",\"attributes.ByAppointmentOnly\",\"attributes.Caters\",\"attributes.RestaurantsReservations\",\"attributes.DietaryRestrictions.dairy-free\",\"attributes.DietaryRestrictions.vegan\",\"attributes.Ambience.romantic\",\"attributes.Music.jukebox\",\"attributes.Ambience.upscale\",\"attributes.RestaurantsTakeOut\",\"attributes.BikeParking\",\"attributes.OutdoorSeating\"]\n",
    "with open('business.csv', 'r') as csvfile, open('restaurant_business.csv', 'w') as outputfile:\n",
    "    reader = csv.DictReader(csvfile, fieldnames=fieldnames)\n",
    "    writer = csv.DictWriter(outputfile, fieldnames=fieldnames)\n",
    "    for row in reader:\n",
    "        if 'Food' in row['categories'] or 'Restaurants' in row['categories']:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "fieldnames_bis = [\"attributes.Ambience.divey\", \"attributes.RestaurantsDelivery\",\"attributes.DogsAllowed\",\"postal_code\",\"hours.Thursday\",\"attributes.HairSpecializesIn.coloring\",\"attributes.BestNights.sunday\",\"attributes.BYOB\",\"attributes.AgesAllowed\",\"attributes.Music.video\",\"hours.Friday\",\"latitude\",\"attributes.Alcohol\",\"attributes.Ambience.classy\",\"attributes.RestaurantsTableService\",\"business_id\",\"attributes.Ambience.touristy\",\"attributes.RestaurantsCounterService\",\"attributes.Corkage\",\"attributes.RestaurantsGoodForGroups\",\"categories\",\"name\",\"attributes.BusinessAcceptsBitcoin\",\"attributes.HappyHour\",\"attributes.WheelchairAccessible\",\"attributes.Ambience.hipster\",\"attributes.BusinessAcceptsCreditCards\",\"is_open\",\"attributes.DietaryRestrictions.vegetarian\",\"attributes.Music.live\",\"attributes.Music.background_music\",\"neighborhood\",\"attributes.BusinessParking.lot\",\"attributes.Music.karaoke\",\"review_count\",\"attributes.GoodForMeal.breakfast\",\"attributes.NoiseLevel\",\"attributes.HairSpecializesIn.perms\",\"state\",\"attributes.DriveThru\",\"attributes.HasTV\",\"attributes.GoodForMeal.dinner\",\"attributes.BusinessParking.street\",\"address\",\"attributes.RestaurantsAttire\",\"hours.Sunday\",\"attributes.BestNights.tuesday\",\"attributes.AcceptsInsurance\",\"attributes.BestNights.wednesday\",\"hours.Wednesday\",\"attributes.HairSpecializesIn.kids\",\"attributes.Open24Hours\",\"attributes.Ambience.trendy\",\"attributes.CoatCheck\",\"hours.Monday\",\"attributes.HairSpecializesIn.straightperms\",\"city\",\"attributes.HairSpecializesIn.curly\",\"attributes.Music.no_music\",\"hours.Tuesday\",\"attributes.HairSpecializesIn.africanamerican\",\"stars\",\"attributes.RestaurantsPriceRange2\",\"attributes.Ambience.intimate\",\"attributes.GoodForMeal.latenight\",\"attributes.GoodForMeal.dessert\",\"attributes.BusinessParking.validated\",\"attributes.GoodForMeal.lunch\",\"attributes.GoodForKids\",\"attributes.DietaryRestrictions.soy-free\",\"attributes.GoodForMeal.brunch\",\"attributes.BusinessParking.valet\",\"longitude\",\"attributes.DietaryRestrictions.gluten-free\",\"attributes.BYOBCorkage\",\"attributes.BusinessParking.garage\",\"attributes.BestNights.friday\",\"hours.Saturday\",\"attributes.Music.dj\",\"attributes.HairSpecializesIn.extensions\",\"attributes.BestNights.saturday\",\"attributes.Ambience.casual\",\"attributes.BestNights.thursday\",\"attributes.BestNights.monday\",\"attributes.HairSpecializesIn.asian\",\"attributes.DietaryRestrictions.kosher\",\"attributes.WiFi\",\"attributes.Smoking\",\"attributes.DietaryRestrictions.halal\",\"attributes.GoodForDancing\",\"attributes.ByAppointmentOnly\",\"attributes.Caters\",\"attributes.RestaurantsReservations\",\"attributes.DietaryRestrictions.dairy-free\",\"attributes.DietaryRestrictions.vegan\",\"attributes.Ambience.romantic\",\"attributes.Music.jukebox\",\"attributes.Ambience.upscale\",\"attributes.RestaurantsTakeOut\",\"attributes.BikeParking\",\"attributes.OutdoorSeating\"]\n",
    "fieldnames_reviews = [\"funny\",\"user_id\",\"review_id\",\"text\",\"business_id\",\"stars\",\"date\",\"useful\",\"cool\"]\n",
    "with open('restaurant_business.csv', 'r') as csvfile, open('review.csv','r') as reviewcsv, open('restaurant_reviews.csv', 'w') as output_rev:\n",
    "    reader_bis = csv.DictReader(csvfile, fieldnames=fieldnames_bis)\n",
    "    reader_rev = csv.DictReader(reviewcsv, fieldnames=fieldnames_reviews)\n",
    "    writer = csv.DictWriter(output_rev, fieldnames=fieldnames_reviews)\n",
    "    for row in reader_bis:\n",
    "        for row_reviews in reader_rev:\n",
    "            if row_reviews['business_id'] == row['business_id']:\n",
    "                writer.writerow(row_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file restaurant_business.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers = [\"attributes.Ambience.divey\", \"attributes.RestaurantsDelivery\",\"attributes.DogsAllowed\",\"postal_code\",\"hours.Thursday\",\"attributes.HairSpecializesIn.coloring\",\"attributes.BestNights.sunday\",\"attributes.BYOB\",\"attributes.AgesAllowed\",\"attributes.Music.video\",\"hours.Friday\",\"latitude\",\"attributes.Alcohol\",\"attributes.Ambience.classy\",\"attributes.RestaurantsTableService\",\"business_id\",\"attributes.Ambience.touristy\",\"attributes.RestaurantsCounterService\",\"attributes.Corkage\",\"attributes.RestaurantsGoodForGroups\",\"categories\",\"name\",\"attributes.BusinessAcceptsBitcoin\",\"attributes.HappyHour\",\"attributes.WheelchairAccessible\",\"attributes.Ambience.hipster\",\"attributes.BusinessAcceptsCreditCards\",\"is_open\",\"attributes.DietaryRestrictions.vegetarian\",\"attributes.Music.live\",\"attributes.Music.background_music\",\"neighborhood\",\"attributes.BusinessParking.lot\",\"attributes.Music.karaoke\",\"review_count\",\"attributes.GoodForMeal.breakfast\",\"attributes.NoiseLevel\",\"attributes.HairSpecializesIn.perms\",\"state\",\"attributes.DriveThru\",\"attributes.HasTV\",\"attributes.GoodForMeal.dinner\",\"attributes.BusinessParking.street\",\"address\",\"attributes.RestaurantsAttire\",\"hours.Sunday\",\"attributes.BestNights.tuesday\",\"attributes.AcceptsInsurance\",\"attributes.BestNights.wednesday\",\"hours.Wednesday\",\"attributes.HairSpecializesIn.kids\",\"attributes.Open24Hours\",\"attributes.Ambience.trendy\",\"attributes.CoatCheck\",\"hours.Monday\",\"attributes.HairSpecializesIn.straightperms\",\"city\",\"attributes.HairSpecializesIn.curly\",\"attributes.Music.no_music\",\"hours.Tuesday\",\"attributes.HairSpecializesIn.africanamerican\",\"stars\",\"attributes.RestaurantsPriceRange2\",\"attributes.Ambience.intimate\",\"attributes.GoodForMeal.latenight\",\"attributes.GoodForMeal.dessert\",\"attributes.BusinessParking.validated\",\"attributes.GoodForMeal.lunch\",\"attributes.GoodForKids\",\"attributes.DietaryRestrictions.soy-free\",\"attributes.GoodForMeal.brunch\",\"attributes.BusinessParking.valet\",\"longitude\",\"attributes.DietaryRestrictions.gluten-free\",\"attributes.BYOBCorkage\",\"attributes.BusinessParking.garage\",\"attributes.BestNights.friday\",\"hours.Saturday\",\"attributes.Music.dj\",\"attributes.HairSpecializesIn.extensions\",\"attributes.BestNights.saturday\",\"attributes.Ambience.casual\",\"attributes.BestNights.thursday\",\"attributes.BestNights.monday\",\"attributes.HairSpecializesIn.asian\",\"attributes.DietaryRestrictions.kosher\",\"attributes.WiFi\",\"attributes.Smoking\",\"attributes.DietaryRestrictions.halal\",\"attributes.GoodForDancing\",\"attributes.ByAppointmentOnly\",\"attributes.Caters\",\"attributes.RestaurantsReservations\",\"attributes.DietaryRestrictions.dairy-free\",\"attributes.DietaryRestrictions.vegan\",\"attributes.Ambience.romantic\",\"attributes.Music.jukebox\",\"attributes.Ambience.upscale\",\"attributes.RestaurantsTakeOut\",\"attributes.BikeParking\",\"attributes.OutdoorSeating\"]\n",
    "data_business = pd.read_csv(\"restaurant_business.csv\", names = headers)\n",
    "data_business.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the file restaurant_reviews.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_review = pd.read_csv(\"reviews_6000.csv\")\n",
    "data_review.head()\n",
    "data_review.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration:\n",
    "##### Concatenate the data based on the common attribute \"business_id\" to get the reviews of the restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_merge = pd.merge(data_review, data_business, on='business_id', how='inner')\n",
    "data_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data_merge[[\"review_id\", \"text\"]]\n",
    "data.head()\n",
    "data.shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "In this task, we extract the sentences from the review text and pre-process it. The preprocessed reviews are represented\n",
    "by 2 techniques such as \n",
    "Bag Of Words and TFIDF. The classifier is built for each of these techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of text reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from iteration_utilities import deepflatten\n",
    "from nltk.stem import RegexpStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "def review_to_words( i, raw_review, dict_ids, j ):\n",
    "    \n",
    "    # 1. Remove tags\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "   \n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z.]\", \" \", review_text) \n",
    "    \n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # 4. Remove the stop words:\n",
    "    stop = []\n",
    "    stop_word = stopwords.words(\"english\")\n",
    "    for i in range(len(stop_word)):\n",
    "        if (stop_word[i] != \"no\") and (stop_word[i] != \"not\") and (stop_word[i] != \"nor\"):\n",
    "            stop.append(stop_word[i])\n",
    "            \n",
    "    stop_words = set(stop)\n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stop_words]\n",
    "            \n",
    "    # 5. Stemming:    \n",
    "    st = RegexpStemmer('s$')\n",
    "    stem = []\n",
    "    for w in meaningful_words:\n",
    "        stem.append(st.stem(w).split())\n",
    "    \n",
    "    stemmed_words = list(deepflatten(stem, depth=1))\n",
    "\n",
    "    # 6. Join the words back into one string separated by space and return the result.\n",
    "    review = \" \".join( stemmed_words )\n",
    "    \n",
    "        \n",
    "    # 7. Split based on period\n",
    "    reviews = []\n",
    "    reviews.append(review.split(\".\"))\n",
    "    review_ids = list(deepflatten(reviews, depth=1))\n",
    "    \n",
    "    for k in range(len(review_ids)):\n",
    "        dict_ids[j] = data[\"review_id\"][i], review_ids[k]\n",
    "        j=j+1\n",
    "        \n",
    "    return j, review.split (\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_ids = {}\n",
    "j = 0\n",
    "\n",
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = data[\"text\"].size\n",
    "\n",
    "# Initialize an empty list to hold the clean reviews or preprocessed reviews\n",
    "clean_reviews = []\n",
    "          \n",
    "for i in range(0, num_reviews):\n",
    "    j, clean_review = review_to_words( i, data[\"text\"][i], dict_ids, j )\n",
    "    clean_reviews.append( clean_review )\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from iteration_utilities import deepflatten\n",
    "\n",
    "clean = list(deepflatten(clean_reviews, depth=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only the useful reviews from the cleaned reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Food words are taken from the reviews. The sentences which contain these words are considered from the cleaned reviews.\n",
    "\n",
    "words =  ['meal', 'tea', 'coffee', 'salad', 'veggie', 'rice', 'teriyaki', 'chicken', 'carrot', 'tomato', 'beef', \n",
    "          'mushroom', 'chilli', 'shrimp', 'egg', 'broccoli', 'curry', 'tofu', 'lettuce', 'puff', 'noodle', 'meat', \n",
    "          'roll', 'wonton', 'bread', 'burger', 'bacon','bun', 'sprout','boba', 'pattie', 'bamboo', 'berry', 'bruschetta',\n",
    "          'cheeto', 'chocolate', 'shake', 'cereal', 'nugget', 'corn', 'crevette','cocktail','daikon', 'donburi','dumpling',\n",
    "          'doughnut', 'frie','fish', 'sandwich', 'cake','taco', 'goyuza', 'hamburger', 'haggi', 'kimmari','kimari', \n",
    "          'kimtchi', 'kimchi', 'lobster','lemonade', 'margarita','milkshake','mochi','milk', 'nacho', 'onigiri','pattie',\n",
    "          'pasta','popcorn', 'pudding','pizza', 'rangoon', 'slider','mustard', 'sausage','spaghetti', 'sappora', \n",
    "          'salmon', 'seaweed','scallop','sansotei','shellfish', 'twinkie', 'zucchini', 'schnitzel', 'macaron', 'brulee', \n",
    "          'steak', 'baco', 'mimosa', 'salmon', 'calamari', 'gnocchi', 'crab', 'chilaquile', 'pancake', 'cocktail', \n",
    "          'buttermilk', 'caesar', 'buffalo', 'broccolini', 'pierogi', 'marshmallow', 'caramel', 'milkshake', 'carolina', \n",
    "          'coleslaw', 'coke', 'cheeseburger', 'banzai', 'swenson', 'bacon', 'hankerin', 'cookie', 'bun', 'ketchup', \n",
    "          'cheesecake', 'brownie',  'onion','anglaise','apricot','avocado','arroncini','bolognese','burrito','beet',\n",
    "          'buffalo','bread','burrata','bacon','blueberry','boar','bun','biscotti','caramel','chimichurri','chestnut',\n",
    "          'chorizo','chipotle','corn','chicken','croquette','cheese','caper','crab','chimi','cotta','clam','cream',\n",
    "          'creme','cheesecake','cod','charcuterie','cider','egg','enchilada','fennel','fajita','fontina','fish','fajita',\n",
    "          'flan','fig','grapefruit','guacamole','gremolata','goat','honey','halibut','haddock','jam','kale','lamb',\n",
    "          'lobster','linguine','latte','mojito','mascarpone','mignonette','marinara','mushroom','mimosa','molacajete',\n",
    "          'musse','nutella','oyster','oreo','orange','oolong','parmesan','pecorino','pasta','pumpkin','polenta','pork',\n",
    "          'peanut','panna','pudding','pistachio','potato','pastry','quesadilla', 'ribollita','radish','risotto',\n",
    "          'ratatouille','rosti','ramekin','ragu','rhubarb','ramekin','salsa','seafood','scotch','steak','sprout','salumi',\n",
    "          'striploin','strawberry','shortrib','sandwiche','smoothie','tequila','tortilla','toritilla','tartare','toast',\n",
    "          'tamale','tenderloin','tagliatelle','toffee','tart', 'vanilla','wonton'\n",
    "         ]\n",
    "\n",
    "useful_reviews = []\n",
    "useful_count = 0\n",
    "dict_review = {}\n",
    "\n",
    "for i in range(0, len(clean)):    \n",
    "    for j in range(0, len(words)):\n",
    "        space = clean[i].split (\" \")  \n",
    "        if words[j] in space:\n",
    "            useful_reviews.append( clean[i] )\n",
    "            break\n",
    "\n",
    "for i in range(len(useful_reviews)):\n",
    "    for k in range(len(dict_ids)):\n",
    "        if useful_reviews[i] in dict_ids[k][1]:\n",
    "            dict_review[i] = dict_ids[k]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afinn Score provides the score in the range of (-5, 5) for every words based on the sentiment.  The total score of the sentence is the sum of afinn score for every word in the sentence. Positive score indicates that review is positive, else it is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "train_labels = []\n",
    "train_data = useful_reviews\n",
    "\n",
    "for i in range(0, len(train_data)):\n",
    "    score = afinn.score(train_data[i])\n",
    "    \n",
    "    res = 'pos' if score > 0 else 'neg'\n",
    "        \n",
    "    train_labels.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the training reviews sentiments in a dictionary 'dict_sentiments'\n",
    "dict_sentiments = {}\n",
    "\n",
    "for i in range(0, len(train_data)):\n",
    "    dict_sentiments[i] = dict_review[i], train_labels[i]\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giving Numerical Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "afinn = Afinn()\n",
    "\n",
    "train_labels_num = []\n",
    "train_data = useful_reviews\n",
    "\n",
    "for i in range(0, len(train_data)):\n",
    "    score = afinn.score(train_data[i])\n",
    "    \n",
    "    res = 1 if score > 0 else 0\n",
    "        \n",
    "    train_labels_num.append(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Representation 1: Bag Of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Models for bag of words representation\n",
    "### 1] SVM\n",
    "### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = { \"C\"      : [1, 6, 10],\n",
    "               \"kernel\" : [\"rbf\", \"linear\", \"poly\", \"sigmoid\"],\n",
    "               \"degree\" : [2, 3, 5, 10],\n",
    "               \"cache_size\" : [100, 200, 300]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = svm, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train2_tfidf, train2_labels)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best parameters given by Grid Search CV are  \n",
    "\"C=1, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=100, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None\"\n",
    "#####  These parameters are considered to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C=1, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None).fit(train_counts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        svm, train_counts, train_labels, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"SVMClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "From the graph of learning curve, it can be inferred that it has Low Variance and Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    " \n",
    "svmc = SVC(C=1, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=True,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None).fit(train_counts, train_labels)\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_counts, train_labels_num, test_size=.25)\n",
    "svm = svmc.fit(X_train, y_train)\n",
    " \n",
    "# Determine the false positive and true positive rates\n",
    "fpr, tpr, _ = roc_curve(y_test, svm.predict_proba(X_test)[:,1])\n",
    " \n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('ROC AUC: %0.2f' % roc_auc)\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2] Random Forest:\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "ran_forest = RandomForestClassifier().fit(train_counts, train_labels)\n",
    "\n",
    "param_grid = { \"n_estimators\"      : [100, 250, 200],\n",
    "               \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "               \"min_samples_split\" : [2, 5, 10] \n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = ran_forest, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 1, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_counts, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=250, criterion='gini', \n",
    "                                min_samples_split = 2).fit(train_counts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        forest, train_counts, train_labels_num, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"RandomForestClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "From the learning curve, it can be inferred that it has High Variance and Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    " \n",
    "forest = RandomForestClassifier(n_estimators=250, criterion='gini', \n",
    "                                min_samples_split = 2)\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_counts, train_labels_num, test_size=.25)\n",
    "forest.fit(X_train, y_train)\n",
    " \n",
    "# Determine the false positive and true positive rates\n",
    "fpr1, tpr1, _ = roc_curve(y_test, forest.predict_proba(X_test)[:,1])\n",
    " \n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr1, tpr1)\n",
    "print('ROC AUC: %0.2f' % roc_auc)\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.plot(fpr1, tpr1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3] Logistic Regression\n",
    "#### Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "lr = LogisticRegression().fit(train_counts, train_labels)\n",
    "\n",
    "param_grid = { \"C\" : [0.5, 0.8, 1, 1.5],\n",
    "               \"intercept_scaling\" : [0.5, 1, 2],\n",
    "               \"solver\" :['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = lr, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_counts, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic = LogisticRegression(C=1.5, intercept_scaling=0.5, solver='liblinear'\n",
    "                             ).fit(train_counts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        logistic, train_counts, train_labels_num, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"LogisticRegressionClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference :\n",
    "From the learning curve, we can infer that this model gives Low Bias and High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    " \n",
    "logistic = LogisticRegression(C=1.5, intercept_scaling=0.5, solver='liblinear')\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_counts, train_labels_num, test_size=.25)\n",
    "log = logistic.fit(X_train, y_train)\n",
    " \n",
    "# Determine the false positive and true positive rates\n",
    "fpr2, tpr2, _ = roc_curve(y_test, log.predict_proba(X_test)[:,1])\n",
    " \n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr2, tpr2)\n",
    "print('ROC AUC: %0.2f' % roc_auc)\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.figure()\n",
    "plt.plot(fpr2, tpr2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "#plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4] AdaBoost\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "ada_boost = AdaBoostClassifier(base_estimator=None, algorithm='SAMME.R', random_state=None)\n",
    "\n",
    "param_grid = {'n_estimators': [50, 80, 100, 150],\n",
    "              'learning_rate': [0.1, 0.5, 0.8, 0.9, 1]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = ada_boost, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_counts, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(base_estimator=None, n_estimators=150, learning_rate=1, algorithm='SAMME.R', \n",
    "                              random_state=None).fit(train_counts, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        adaboost, train_counts, train_labels_num, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"AdaBoostClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "From the Learning Curve, it can be inferred that adaboost classifier has Low variance and Low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    " \n",
    "adaboost = AdaBoostClassifier(base_estimator=None, n_estimators=150, learning_rate=1, algorithm='SAMME.R', \n",
    "                              random_state=None)    \n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_counts, train_labels_num, test_size=.25)\n",
    "ada = adaboost.fit(X_train, y_train)\n",
    " \n",
    "# Determine the false positive and true positive rates\n",
    "fpr3, tpr3, _ = roc_curve(y_test, ada.predict_proba(X_test)[:,1])\n",
    " \n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr3, tpr3)\n",
    "print('ROC AUC: %0.2f' % roc_auc)\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "plt.plot(fpr3, tpr3, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Representation 2: Tfidf [Term Frequency Inverse Document Frequency]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "train_tfidf = tfidf_transformer.fit_transform(train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Models\n",
    "### 1] SVM\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = { \"C\"      : [1, 6, 10],\n",
    "               \"kernel\" : [\"rbf\", \"linear\", \"poly\", \"sigmoid\"],\n",
    "               \"degree\" : [2, 3, 5, 10],\n",
    "               \"cache_size\" : [100, 200, 300]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = svm, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_tfidf, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best parameters given by Grid Search CV are  \n",
    "\"C = 6, cache_size = 100, degree = 2, kernel = 'linear'\"\n",
    "#####  These parameters are considered to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C = 6, cache_size = 100, degree = 2, kernel = 'linear').fit(train_tfidf, train_labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "svm = SVC(C = 6, cache_size = 100, degree = 2, kernel = 'linear').fit(train_tfidf, train_labels_num)\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        svm, train_tfidf, train_labels_num, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "\n",
    "plt.title(\"SVMClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference :\n",
    "From the Learning Curve, it can be inferred that SVM classifier has Low Variance and Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curve for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.cross_validation import train_test_split\n",
    " \n",
    "svmc = SVC(C = 6, cache_size = 100, degree = 2, kernel = 'linear').fit(train_tfidf, train_labels_num)\n",
    "\n",
    "# shuffle and split training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_counts, train_labels_num, test_size=.25)\n",
    "svm = svmc.fit(X_train, y_train)\n",
    " \n",
    "# Determine the false positive and true positive rates\n",
    "fpr, tpr, _ = roc_curve(y_test, svm.predict_proba(X_test)[:,1])\n",
    " \n",
    "# Calculate the AUC\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('ROC AUC: %0.2f' % roc_auc)\n",
    " \n",
    "# Plot of a ROC curve for a specific class\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2] Random Forest\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "ran_forest = RandomForestClassifier()\n",
    "\n",
    "param_grid = { \"n_estimators\"      : [100, 250, 200],\n",
    "               \"criterion\"         : [\"gini\", \"entropy\"],\n",
    "               \"min_samples_split\" : [2, 5, 10] \n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = ran_forest, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_tfidf, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(criterion = 'gini', min_samples_split = 5, n_estimators = 200).fit(train_tfidf, train_labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "forest = RandomForestClassifier(criterion = 'gini', min_samples_split = 5, n_estimators = 200).fit(train_tfidf, train_labels)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        forest, train_tfidf, train_labels_num, cv=10, n_jobs=4, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"RandomForestClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference:\n",
    "From the Learning Curve, it can be inferred that Random Forest classifier has Low Bias, High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3] Logistic Regression\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "lr = LogisticRegression()\n",
    "\n",
    "param_grid = { \"C\" : [0.5, 0.8, 1, 1.5],\n",
    "               \"intercept_scaling\" : [0.5, 1, 2],\n",
    "               \"solver\" :['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = lr, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_tfidf, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(C = 1.5, intercept_scaling = 0.5, solver = 'lbfgs').fit(train_tfidf, train_labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "logistc = LogisticRegression(C = 1.5, intercept_scaling = 0.5, solver = 'liblinear').fit(train_tfidf, train_labels)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        logistic, train_tfidf, train_labels_num, cv=10, n_jobs=4, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"LogisticRegressionClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference :\n",
    "From the Learning Curve, it can be inferred that Logistic Regression classifier has Low Bias, High Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4] Ada Boost\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "ada_boost = AdaBoostClassifier(base_estimator=None, algorithm='SAMME.R', random_state=None)\n",
    "\n",
    "param_grid = {'n_estimators': [50, 80, 100, 150],\n",
    "              'learning_rate': [0.1, 0.5, 0.8, 0.9, 1]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = ada_boost, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train_tfidf, train_labels_num)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(learning_rate = 1, n_estimators = 150).fit(train_tfidf, train_labels_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "adaboost = AdaBoostClassifier(learning_rate = 1, n_estimators = 150).fit(train_tfidf, train_labels_num)\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        adaboost, train_tfidf, train_labels_num, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"AdaBoostClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.6, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference :\n",
    "From the Learning Curve, it can be inferred that Ada Boost classifier has Low Variance, Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "###### In this task we consider the preprocessed text reviews for classification of reviews into 4 courses. The 4 courses are:\n",
    "      1]Soups and Salads \n",
    "      2]Appetizers\n",
    "      3]Main Course\n",
    "      4]Dessserts\n",
    "###### The data is represented in two different ways such as one hot encoding and TFIDF representation. After 4 course meal classsification, \n",
    "###### We predict most preferred food items in each of the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Training Data\n",
    "The food words are classified into 4 courses. These course names are given as labels for the review which contains respective  food words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words =  set(['meal', 'tea', 'coffee', 'salad', 'veggie', 'rice', 'teriyaki', 'chicken', 'carrot', 'tomato', 'beef', \n",
    "          'mushroom', 'chilli', 'shrimp', 'egg', 'broccoli', 'curry', 'tofu', 'lettuce', 'puff', 'noodle', 'meat', \n",
    "          'roll', 'wonton', 'bread', 'burger', 'bacon','bun', 'sprout','boba', 'pattie', 'bamboo', 'berry', 'bruschetta',\n",
    "          'cheeto', 'chocolate', 'shake', 'cereal', 'nugget', 'corn', 'crevette','cocktail','daikon', 'donburi','dumpling',\n",
    "          'doughnut', 'frie','fish', 'sandwich', 'cake','taco', 'goyuza', 'hamburger', 'haggi', 'kimmari','kimari', \n",
    "          'kimtchi', 'kimchi', 'lobster','lemonade', 'margarita','milkshake','mochi','milk', 'nacho', 'onigiri','pattie',\n",
    "          'pasta','popcorn', 'pudding','pizza', 'rangoon', 'slider','mustard', 'sausage','spaghetti', 'sappora', \n",
    "          'salmon', 'seaweed','scallop','sansotei','shellfish', 'twinkie', 'zucchini', 'schnitzel', 'macaron', 'brulee', \n",
    "          'steak', 'baco', 'mimosa', 'salmon', 'calamari', 'gnocchi', 'crab', 'chilaquile', 'pancake', 'cocktail', \n",
    "          'buttermilk', 'caesar', 'buffalo', 'broccolini', 'pierogi', 'marshmallow', 'caramel', 'milkshake', 'carolina', \n",
    "          'coleslaw', 'coke', 'cheeseburger', 'banzai', 'swenson', 'bacon', 'hankerin', 'cookie', 'bun', 'ketchup', \n",
    "          'cheesecake', 'brownie',  'onion','anglaise','apricot','avocado','arroncini','bolognese','burrito','beet',\n",
    "          'buffalo','bread','burrata','bacon','blueberry','boar','bun','biscotti','caramel','chimichurri','chestnut',\n",
    "          'chorizo','chipotle','corn','chicken','croquette','cheese','caper','crab','chimi','cotta','clam','cream',\n",
    "          'creme','cheesecake','cod','charcuterie','cider','egg','enchilada','fennel','fajita','fontina','fish','fajita',\n",
    "          'flan','fig','grapefruit','guacamole','gremolata','goat','honey','halibut','haddock','jam','kale','lamb',\n",
    "          'lobster','linguine','latte','mojito','mascarpone','mignonette','marinara','mushroom','mimosa','molacajete',\n",
    "          'musse','nutella','oyster','oreo','orange','oolong','parmesan','pecorino','pasta','pumpkin','polenta','pork',\n",
    "          'peanut','panna','pudding','pistachio','potato','pastry','quesadilla', 'ribollita','radish','risotto',\n",
    "          'ratatouille','rosti','ramekin','ragu','rhubarb','ramekin','salsa','seafood','scotch','steak','sprout','salumi',\n",
    "          'striploin','strawberry','shortrib','sandwiche','smoothie','tequila','tortilla','toritilla','tartare','toast',\n",
    "          'tamale','tenderloin','tagliatelle','toffee','tart', 'vanilla','wonton'\n",
    "         ])\n",
    "\n",
    "main_course = ['meal', 'fish', 'rice', 'teriyaki', 'chicken', 'beef', 'curry', 'tofu', 'meat', 'bacon','crevette', 'donburi', 'lobster', 'mochi', 'onigiri', 'anglaise','bolognese','buffalo','boar','beef','chipotle','chicken','croquette','crab','chimi','enchilada','fajita','fontina','fish','fajita','gremolata','goat','lamb','lobster','linguine','mimosa','polenta','pork','quesadilla','ramekin','ragu','steak','salumi','striploin','shortrib','tamale','tenderloin','tagliatelle','wonton']\n",
    "soups = ['tea', 'coffee', 'salad', 'veggie', 'carrot', 'tomato', 'chilli',  'broccoli', 'lettuce', 'boba', 'berry', 'corn', 'cocktail', 'lemonade', 'margarita', 'mustard', 'beet','chestnut','corn','cider','coffee','fennel','kale','lemon','latte','mojito','mascarpone','oolong','pomegranate','parmesan','pumpkin','ribollita','radish','sprout','tequila','tea','tomato']\n",
    "desserts = ['chocolate', 'shake', 'doughnut', 'cake', 'haggi', 'milkshake', 'milk', 'pudding', 'apple','apricot','avocado','banana','blueberry','caramel','cream','creme','cheesecake','flan','fig','grapefruit','guacamole','honey','jam','molacajete','mousse','milkshake','nutella','oreo','orange','panna','pudding','pistachio','pastry','rhubarb','ramekin','strawberry','smoothie','toffee','tart','vanilla']\n",
    "appetizers = ['egg', 'shrimp', 'puff', 'roll', 'wonton', 'bread', 'burger', 'mushroom', 'bun', 'sprout', 'pattie', 'bamboo', 'bruschetta', 'cheeto', 'cereal', 'nugget', 'daikon', 'dumpling', 'noodle', 'frie', 'sandwich', 'taco', 'goyuza', 'hamburger', 'kimmari','kimari', 'kimtchi', 'kimchi', 'nacho', 'pattie','pasta','popcorn','pizza', 'rangoon', 'slider', 'sausage', 'spaghetti', 'arroncini','butter','burrito','burrito','bread','bun','burrata','bacon','biscotti','charcuterie','chimichurri','chorizo','cheese','caper','cotta','clam','cod','egg','halibut','haddock','mignonette','marinara','mushroom','oyster','pecorino','pasta','peanut','potatoe','risotto','ratatouille','rosti','salsa','sandwiche','seafood','scotch','tortilla','toritilla','tartare','toast']\n",
    "     \n",
    "train2_labels = []\n",
    "train2_data = dict_sentiments\n",
    "\n",
    "\n",
    "for i in range(len(train2_data)):\n",
    "    space = train2_data[i][0][1].split (\" \")\n",
    "    label = \"appetizers\"\n",
    "    for j in range(len(main_course)):\n",
    "        if main_course[j] in space:\n",
    "            label = \"main_course\"\n",
    "           \n",
    "    for j in range(len(soups)):\n",
    "        if soups[j] in space:\n",
    "            label = \"soups_and_salads\"\n",
    "           \n",
    "    for j in range(len(desserts)):\n",
    "        if desserts[j] in space:\n",
    "            label = \"desserts\"\n",
    "\n",
    "   \n",
    "    train2_labels.append(label)\n",
    "\n",
    "print(train2_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2 = []\n",
    " \n",
    "for i in range(len(train2_data)):\n",
    "    word_list = []\n",
    "    space = train2_data[i][0][1].split (\" \")\n",
    "    for j in range(len(words)):        \n",
    "        if words[j] in space:\n",
    "            word_list.append(words[j])\n",
    "    #print(i, word_list)\n",
    "    word = \" \".join( word_list )\n",
    "    #print(word)\n",
    "    train2.append(word)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2_data_2 = []\n",
    " \n",
    "for i in range(len(train2_data)):\n",
    "    space = train2_data[i][0][1].split (\" \")\n",
    "    for j in range(len(words)):        \n",
    "        if words[j] in space:\n",
    "            train2_data_2.append(train2_data[i][0][1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation 1: One Hot Encoding\n",
    "Categorical data is converted into numerical representation by creating sparse matrix taking rows as reviews and columns\n",
    "as food words. Matrix Value will be 1 if review contains food words, else 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "values = array(words)\n",
    "\n",
    "b = len(train2_data)\n",
    "c = len(words)\n",
    "a=np.zeros((b, c))\n",
    " \n",
    "for i in range(0, len(train2_data)):        \n",
    "    for j in range(0, len(words)-1):\n",
    "        if words[j] in train2_data[i][0][1]:\n",
    "            a[i][j]=1\n",
    "        else:\n",
    "            a[i][j]=0\n",
    "            \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert output matrix 'a' into dataframe.\n",
    "\n",
    "df_train = pd.DataFrame(a, columns = ['meal', 'tea', 'coffee', 'salad', 'veggie', 'rice', 'teriyaki', 'chicken', 'carrot', 'tomato', 'beef', \n",
    "          'mushroom', 'chilli', 'shrimp', 'egg', 'broccoli', 'curry', 'tofu', 'lettuce', 'puff', 'noodle', 'meat', \n",
    "          'roll', 'wonton', 'bread', 'burger', 'bacon','bun', 'sprout','boba', 'pattie', 'bamboo', 'berry', 'bruschetta',\n",
    "          'cheeto', 'chocolate', 'shake', 'cereal', 'nugget', 'corn', 'crevette','cocktail','daikon', 'donburi','dumpling',\n",
    "          'doughnut', 'frie','fish', 'sandwich', 'cake','taco', 'goyuza', 'hamburger', 'haggi', 'kimmari','kimari', \n",
    "          'kimtchi', 'kimchi', 'lobster','lemonade', 'margarita','milkshake','mochi','milk', 'nacho', 'onigiri','pattie',\n",
    "          'pasta','popcorn', 'pudding','pizza', 'rangoon', 'slider','mustard', 'sausage','spaghetti', 'sappora', \n",
    "          'salmon', 'seaweed','scallop','sansotei','shellfish', 'twinkie', 'zucchini', 'schnitzel', 'macaron', 'brulee', \n",
    "          'steak', 'baco', 'mimosa', 'salmon', 'calamari', 'gnocchi', 'crab', 'chilaquile', 'pancake', 'cocktail', \n",
    "          'buttermilk', 'caesar', 'buffalo', 'broccolini', 'pierogi', 'marshmallow', 'caramel', 'milkshake', 'carolina', \n",
    "          'coleslaw', 'coke', 'cheeseburger', 'banzai', 'swenson', 'bacon', 'hankerin', 'cookie', 'bun', 'ketchup', \n",
    "          'cheesecake', 'brownie',  'onion','anglaise','apricot','avocado','arroncini','bolognese','burrito','beet',\n",
    "          'buffalo','bread','burrata','bacon','blueberry','boar','bun','biscotti','caramel','chimichurri','chestnut',\n",
    "          'chorizo','chipotle','corn','chicken','croquette','cheese','caper','crab','chimi','cotta','clam','cream',\n",
    "          'creme','cheesecake','cod','charcuterie','cider','egg','enchilada','fennel','fajita','fontina','fish','fajita',\n",
    "          'flan','fig','grapefruit','guacamole','gremolata','goat','honey','halibut','haddock','jam','kale','lamb',\n",
    "          'lobster','linguine','latte','mojito','mascarpone','mignonette','marinara','mushroom','mimosa','molacajete',\n",
    "          'musse','nutella','oyster','oreo','orange','oolong','parmesan','pecorino','pasta','pumpkin','polenta','pork',\n",
    "          'peanut','panna','pudding','pistachio','potato','pastry','quesadilla', 'ribollita','radish','risotto',\n",
    "          'ratatouille','rosti','ramekin','ragu','rhubarb','ramekin','salsa','seafood','scotch','steak','sprout','salumi',\n",
    "          'striploin','strawberry','shortrib','sandwiche','smoothie','tequila','tortilla','toritilla','tartare','toast',\n",
    "          'tamale','tenderloin','tagliatelle','toffee','tart', 'vanilla','wonton'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train2_label = le.fit_transform(train2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models\n",
    "### SVM\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "## Replace Logistic\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = { \"C\"      : [1, 6, 10],\n",
    "               \"kernel\" : [\"rbf\", \"linear\", \"poly\", \"sigmoid\"],\n",
    "               \"degree\" : [2, 3, 5, 10],\n",
    "               \"cache_size\" : [100, 200, 300]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = svm, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(df_train, train2_label)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best parameters given by Grid Search CV are  \n",
    "\"C=6, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,tol=0.001, cache_size=100, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None\"\n",
    "#####  These parameters are considered to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C=6, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None).fit(df_train, train2_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        svm, df_train, train2_label, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"SVMClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "From the graph of learning curve, it can be inferred that it has Low Variance and Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Representation 2: TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train2_counts = count_vect.fit_transform(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "train2_tfidf = tfidf_transformer.fit_transform(train2_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Model\n",
    "### SVM\n",
    "#### Parameter Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import decomposition, grid_search\n",
    "\n",
    "\n",
    "#Grid Search - Used to find the best combination of parameters\n",
    "svm = SVC()\n",
    "\n",
    "param_grid = { \"C\"      : [1, 6, 10],\n",
    "               \"kernel\" : [\"rbf\", \"linear\", \"poly\", \"sigmoid\"],\n",
    "               \"degree\" : [2, 3, 5, 10],\n",
    "               \"cache_size\" : [100, 200, 300]\n",
    "             }\n",
    "\n",
    "model = grid_search.GridSearchCV(estimator = svm, param_grid = param_grid, scoring = 'accuracy', verbose = 2, n_jobs = 4, iid = True, refit = True, cv=10)\n",
    "\n",
    "model.fit(train2_tfidf, train2_labels)\n",
    "print (\"Best score: %0.3f\" % model.best_score_)\n",
    "print (\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The best parameters given by Grid Search CV are  \n",
    "\"C=10, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=100, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None\"\n",
    "#####  These parameters are considered to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(C=10, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None).fit(train2_tfidf, train2_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.learning_curve import learning_curve\n",
    " \n",
    "# assume classifier and training data is prepared...\n",
    " \n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "        svm, train2_tfidf, train2_labels, cv=10, n_jobs=-1, train_sizes=np.linspace(.1, 1., 10), verbose=0)\n",
    " \n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    " \n",
    "plt.figure()\n",
    "plt.title(\"SVMClassifier\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim((0.0, 1.01))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid()\n",
    " \n",
    "# Plot the average training and test score lines at each training set size\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"r\", label=\"Test score\")\n",
    " \n",
    "# Plot the std deviation as a transparent range at each training set size\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, \n",
    "                 alpha=0.1, color=\"b\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, \n",
    "                 alpha=0.1, color=\"r\")\n",
    " \n",
    "# Draw the plot and reset the y-axis\n",
    "plt.draw()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "From the graph of learning curve, it can be inferred that it has Low Variance and Low Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A similar appraoch is carried out for other models such as AdaBoost, Logistic Regression, Random Forest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "### Task 1\n",
    "#### Technique 1: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "clf1 = SVC(C=1, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "scores1 = cross_val_score(clf1, train_counts, train_labels_num, cv=5)\n",
    "\n",
    "clf2 = SVC(C = 1, cache_size = 100, degree = 2, kernel = 'linear').fit(train_tfidf, train_labels_num)\n",
    "scores2 = cross_val_score(clf2, train_tfidf, train_labels_num, cv=5)\n",
    "\n",
    "print(scores1) \n",
    "print(scores2)\n",
    "\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    avg1 = avg1 + scores1[i]\n",
    "    avg2 = avg2 + scores2[i]\n",
    "    \n",
    "avg1 = avg1 / 5\n",
    "avg2 = avg2/5\n",
    "    \n",
    "print(\"Average Accuracy of Bag Of Words representation :  \", avg1)    \n",
    "print(\"Average Accuracy of TFIDF representation :  \", avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique 2: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier(n_estimators=200, criterion='entropy', \n",
    "                                min_samples_split = 2)\n",
    "scores1 = cross_val_score(clf1, train_counts, train_labels_num, cv=5)\n",
    "\n",
    "clf2 = RandomForestClassifier(criterion = 'entropy', min_samples_split = 2, n_estimators = 200)\n",
    "scores2 = cross_val_score(clf2, train_tfidf, train_labels_num, cv=5)\n",
    "\n",
    "print(scores1) \n",
    "print(scores2) \n",
    "\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    avg1 = avg1 + scores1[i]\n",
    "    avg2 = avg2 + scores2[i]\n",
    "    \n",
    "avg1 = avg1 / 5\n",
    "avg2 = avg2/5\n",
    "    \n",
    "print(\"Average Accuracy of Preprocessing 1 :  \", avg1)    \n",
    "print(\"Average Accuracy of Preprocessing 2 :  \", avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf1 = LogisticRegression(C=1.5, intercept_scaling=0.5, solver='newton-cg')\n",
    "scores1 = cross_val_score(clf1, train_counts, train_labels_num, cv=5)\n",
    "\n",
    "clf2 = LogisticRegression(C = 1.5, intercept_scaling = 0.5, solver = 'newton-cg')\n",
    "scores2 = cross_val_score(clf2, train_tfidf, train_labels_num, cv=5)\n",
    "\n",
    "print(scores1) \n",
    "print(scores2)\n",
    "\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    avg1 = avg1 + scores1[i]\n",
    "    avg2 = avg2 + scores2[i]\n",
    "    \n",
    "avg1 = avg1 / 5\n",
    "avg2 = avg2/5\n",
    "    \n",
    "print(\"Average Accuracy of Preprocessing 1 :  \", avg1)    \n",
    "print(\"Average Accuracy of Preprocessing 2 :  \", avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technique 4: AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = AdaBoostClassifier(base_estimator=None, n_estimators=150, learning_rate=1, algorithm='SAMME.R',random_state=None)\n",
    "scores1 = cross_val_score(clf1, train_counts, train_labels_num, cv=5)\n",
    "\n",
    "clf2 = AdaBoostClassifier(learning_rate = 0.8, n_estimators = 150).fit(train_tfidf, train_labels_num)\n",
    "scores2 = cross_val_score(clf2, train_tfidf, train_labels_num, cv=5)\n",
    "\n",
    "print(scores1) \n",
    "print(scores2)\n",
    "\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    avg1 = avg1 + scores1[i]\n",
    "    avg2 = avg2 + scores2[i]\n",
    "    \n",
    "avg1 = avg1 / 5\n",
    "avg2 = avg2/5\n",
    "    \n",
    "print(\"Average Accuracy of Preprocessing 1 :  \", avg1)    \n",
    "print(\"Average Accuracy of Preprocessing 2 :  \", avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inference :\n",
    "We can observe that SVM Technique performs better than other 3 techniques. Bag of words representation performs better than TFIDF. Hence SVM model for Bag of words representation is considered for task1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "### Cross Validation of svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf1 = SVC(C=1, kernel='linear', degree=2, gamma=1, coef0=0.0, shrinking=True, probability=False,\n",
    "          tol=0.001, cache_size=100, class_weight=None,\n",
    "          verbose=False, max_iter=-1, decision_function_shape='ovr', random_state=None)\n",
    "scores1 = cross_val_score(clf1, train_counts, train_labels_num, cv=5)\n",
    "\n",
    "clf2 = SVC(C = 1, cache_size = 100, degree = 2, kernel = 'linear').fit(train_tfidf, train_labels_num)\n",
    "scores2 = cross_val_score(clf2, train_tfidf, train_labels_num, cv=5)\n",
    "\n",
    "print(scores1) \n",
    "print(scores2)\n",
    "\n",
    "avg1 = 0\n",
    "avg2 = 0\n",
    "\n",
    "for i in range(5):\n",
    "    avg1 = avg1 + scores1[i]\n",
    "    avg2 = avg2 + scores2[i]\n",
    "    \n",
    "avg1 = avg1 / 5\n",
    "avg2 = avg2/5\n",
    "    \n",
    "print(\"Average Accuracy of one hot encoding :  \", avg1)    \n",
    "print(\"Average Accuracy of TFIDF :  \", avg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inference :\n",
    "We can observe that SVM Technique performs better than other 3 techniques. TFIDF  representation performs better than one hot encoding. Hence TFIDF representation is considered for task 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the top items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vec_pipe = TfidfVectorizer()\n",
    "Xtr = vec_pipe.fit_transform(train2)\n",
    "features = vec_pipe.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=5):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n=5):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_feats_in_doc(Xtr, features, 2, top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_reviews = []\n",
    "neg_reviews = []\n",
    "dict_pos = {}\n",
    "dict_neg = {}\n",
    "j=0\n",
    "k=0\n",
    "\n",
    "for i in range(len(train2_data)):\n",
    "    if train2_data[i][1] == \"pos\":\n",
    "        pos_reviews.append(train2[i])\n",
    "        dict_pos[i] = train2_data[i]\n",
    "        j=j+1\n",
    "    else:\n",
    "        neg_reviews.append(train2[i])  \n",
    "        dict_neg[i] = train2_data[i]\n",
    "        k=k+1\n",
    "        \n",
    "pos_counts = count_vect.fit_transform(pos_reviews)\n",
    "neg_counts = count_vect.fit_transform(neg_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 items in positive category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = vec_pipe.get_feature_names()\n",
    "\n",
    "feature_array = np.array(vec_pipe.get_feature_names())\n",
    "tfidf_sorting = np.argsort(pos_counts.toarray()).flatten()[::-1]\n",
    "\n",
    "n = 100\n",
    "top_n = feature_array[tfidf_sorting][:n]\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_main_course = []\n",
    "top_soups = []\n",
    "top_appetizers = []\n",
    "top_desserts = []\n",
    "count_main = 0\n",
    "count_soups = 0;\n",
    "count_appetizers = 0\n",
    "count_desserts = 0\n",
    "\n",
    "for i in range(len(top_n)):\n",
    "    if count_main < 5:\n",
    "        for j in range(len(main_course)):\n",
    "            if top_n[i] in main_course[j]:\n",
    "                top_main_course.append(top_n[i])\n",
    "                count_main = count_main + 1\n",
    "                break\n",
    "    if count_soups < 5:\n",
    "        for j in range(len(soups)):\n",
    "            if top_n[i] in soups[j]:\n",
    "                top_soups.append(top_n[i])\n",
    "                count_soups = count_soups + 1\n",
    "                break\n",
    "    if count_desserts < 5:            \n",
    "        for j in range(len(desserts)):\n",
    "            if top_n[i] in desserts[j]:\n",
    "                top_desserts.append(top_n[i])\n",
    "                count_desserts = count_desserts + 1\n",
    "                break\n",
    "    if count_appetizers < 5:        \n",
    "        for j in range(len(appetizers)):\n",
    "            if top_n[i] in appetizers[j]:\n",
    "                top_appetizers.append(top_n[i])\n",
    "                count_appetizers = count_appetizers + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_main_course)\n",
    "print(top_soups)\n",
    "print(top_desserts)\n",
    "print(top_appetizers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
